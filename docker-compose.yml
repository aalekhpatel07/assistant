version: "3.8"
services:
  tts: # speech to text inference server.
    image: coqui-ai-tts
    build:
      context: docker/coqui-tts
      dockerfile: coqui-tts.Dockerfile
    restart: unless-stopped
    volumes:
      - model_data:/root/.local/share/tts
        # - ./docker/coqui-tts/server:/root/TTS/server
    ports:
      - "5002:5002"
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]

  redis:
    image: redis:6.2-alpine
    restart: unless-stopped
    ports:
      - '6380:6379'
    volumes:
      - cache:/data

  stt:
    image: onerahmet/openai-whisper-asr-webservice:latest-gpu
    restart: unless-stopped
    ports:
      - "9003:9000"
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    environment:
      - ASR_MODEL=base
      - ASR_ENGINE=openai_whisper
    volumes:
      - stt-cache:/root/.cache/whisper


  llama:
    build:
      context: .
      dockerfile: docker/llama.Dockerfile
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu, compute, utility]
    restart: unless-stopped
    ports:
      - 5001:8080
    volumes:
      - ./docker/llama:/app
      - ./docker/llama/models:/models
    ulimits:
      memlock:
        soft: -1
        hard: -1
    command: python3 -m llama_cpp.server --config_file llama_config.json

volumes:
  model_data:
  cache:
  stt-cache:

