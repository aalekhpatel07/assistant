version: "3.8"
services:
  congo:
    image: congo
    build:
      context: .
      dockerfile: docker/driver.Dockerfile
    restart: unless-stopped
    environment:
      - COQUI_BASE_URL="http://stt:5002"
      - REDIS_ADDR="redis:6379"
      - WHISPER_BASE_URL="http://tts:9000"
      - LLAMA_PHI_BASE_URL="http://llama:8080"
    privileged: true
    devices:
      - /dev/snd:/dev/snd

  stt: # text to speech inference server.
    image: coqui-ai-tts
    build:
      context: docker/coqui-tts
      dockerfile: coqui-tts.Dockerfile
    restart: unless-stopped
    volumes:
      - model_data:/root/.local/share/tts
    ports:
      - 5002
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]

  redis:
    image: redis:6.2-alpine
    restart: unless-stopped
    ports:
      - 6379
    volumes:
      - cache:/data

  tts:
    image: onerahmet/openai-whisper-asr-webservice:latest-gpu
    restart: unless-stopped
    ports:
      - 9000
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    environment:
      - ASR_MODEL=base
      - ASR_ENGINE=openai_whisper
    volumes:
      - stt-cache:/root/.cache/whisper


  llama:
    build:
      context: .
      dockerfile: docker/llama.Dockerfile
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu, compute, utility]
    restart: unless-stopped
    ports:
      - 8080
    volumes:
      - ./docker/llama:/app
      - ./docker/llama/models:/models
    ulimits:
      memlock:
        soft: -1
        hard: -1
    command: python3 -m llama_cpp.server --config_file llama_config.json

volumes:
  model_data:
  cache:
  stt-cache:

